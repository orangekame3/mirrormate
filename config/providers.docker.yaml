providers:
  llm:
    enabled: true
    provider: ollama
    openai:
      model: gpt-4o-mini
      maxTokens: 300
      temperature: 0.7
    ollama:
      model: hf.co/rinna/qwen2.5-bakeneko-32b-instruct-gguf:Q8_0
      baseUrl: "http://host.docker.internal:11434"
      maxTokens: 300
      temperature: 0.7

  tts:
    enabled: true
    provider: voicevox
    openai:
      voice: shimmer
      model: tts-1
      speed: 0.95
    voicevox:
      speaker: 2
      baseUrl: "http://voicevox:50021"

  embedding:
    enabled: true
    provider: ollama
    ollama:
      model: bge-m3
      baseUrl: "http://host.docker.internal:11434"

  memory:
    enabled: true
    rag:
      topK: 8
      threshold: 0.3
    extraction:
      autoExtract: true
      minConfidence: 0.5
