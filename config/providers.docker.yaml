providers:
  llm:
    enabled: true
    provider: ollama
    openai:
      model: gpt-4o-mini
      maxTokens: 300
      temperature: 0.7
    ollama:
      model: "qwen2.5:14b"
      baseUrl: "http://host.docker.internal:11434"
      maxTokens: 300
      temperature: 0.7

  tts:
    enabled: true
    provider: voicevox
    openai:
      voice: shimmer
      model: tts-1
      speed: 0.95
    voicevox:
      speaker: 2
      baseUrl: "http://voicevox:50021"
